{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME']='/usr/hdp/current/spark2-client'\n",
    "os.environ['HADOOP_CONF_DIR']='/etc/hadoop/conf'\n",
    "os.environ['JAVA_HOME']='/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b10-0.el7_6.x86_64'\n",
    "os.environ['PYSPARK_PYTHON']='/bin/python3.6'\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "#PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobName='order_det_summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(jobName).enableHiveSupport().\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing started\n",
      "count of orders:68883\n",
      "count of order items:172198\n",
      "processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"processing started\")\n",
    "\n",
    "oiDf=spark.read.json(\"/public/retail_db_json/order_items\")\n",
    "oDf=spark.read.json(\"/public/retail_db_json/orders\")\n",
    "\n",
    "print(f\"count of orders:{oDf.count()}\")\n",
    "print(f\"count of order items:{oiDf.count()}\")\n",
    "\n",
    "orders=oiDf.join(oDf,oiDf.order_item_id==oDf.order_id,'left').select([*oiDf.columns,'order_customer_id','order_status'])\n",
    "\n",
    "#Processed order summarization\n",
    "\n",
    "\n",
    "#Order summary per status\n",
    "orders_sum_to_status=orders.groupBy(\"order_status\").\\\n",
    "agg(F.sum(\"order_item_subtotal\").cast(DecimalType(18, 2)).alias(\"total_amt\"),F.count(\"order_item_id\").alias(\"order_items_total\")).\\\n",
    "withColumn(\"order_status\",F.when(F.col(\"order_status\").isNull(),F.lit(\"EXCEPTION\")).otherwise(F.col(\"order_status\")))\n",
    "\n",
    "#order summary per product\n",
    "orders_sum_to_product=orders.where(\"order_status is not null\").groupBy(\"order_item_product_id\").\\\n",
    "agg(F.sum(\"order_item_subtotal\").cast(DecimalType(18, 2)).alias(\"total_amt\"),F.sum(\"order_item_quantity\").alias(\"tot_qty\"))\n",
    "\n",
    "#order summary per customer\n",
    "orders_sum_to_customer=orders.where(\"order_status is not null\").groupBy(\"order_customer_id\").\\\n",
    "agg(F.sum(\"order_item_subtotal\").cast(DecimalType(18, 2)).alias(\"total_amt\"))\n",
    "\n",
    "#order summary per customer for each product\n",
    "orders_sum_to_customer_product=orders.where(\"order_status is not null\").groupBy(\"order_customer_id\",\"order_item_product_id\").\\\n",
    "agg(F.sum(\"order_item_subtotal\").cast(DecimalType(18, 2)).alias(\"total_amt\"),F.sum(\"order_item_quantity\").alias(\"tot_qty\"))\n",
    "\n",
    "#order summary per customer for each status\n",
    "orders_sum_to_customer_status=orders.where(\"order_status is not null\").groupBy(\"order_customer_id\",\"order_status\").\\\n",
    "agg(F.count(\"order_item_order_id\").alias(\"orders_total\"),F.sum(\"order_item_subtotal\").cast(DecimalType(18, 2)).alias(\"total_amt\"))\n",
    "\n",
    "\n",
    "#Exception orders\n",
    "orders_exception=orders.where(\"order_status is null\").\\\n",
    "withColumn('order_status',F.lit('EXCEPTION')).drop(\"order_customer_id\")\n",
    "\n",
    "#Exception order summary\n",
    "orders_ex_sum_to_prod=orders_exception.groupBy(\"order_item_product_id\").\\\n",
    "agg(F.sum(\"order_item_subtotal\").cast(DecimalType(18, 2)).alias(\"total_amt\"),F.sum(\"order_item_quantity\").alias(\"tot_qty\"))\n",
    "\n",
    "#wrting aggregated tables to hive\n",
    "\n",
    "\n",
    "orders_sum_to_status.write.mode(\"overwrite\").saveAsTable(\"pathirippilly_db.orders_sum_to_status\")\n",
    "orders_sum_to_product.write.mode(\"overwrite\").saveAsTable(\"pathirippilly_db.orders_sum_to_product\")\n",
    "orders_sum_to_customer.write.mode(\"overwrite\").saveAsTable(\"pathirippilly_db.orders_sum_to_customer\")\n",
    "orders_sum_to_customer_product.write.mode(\"overwrite\").saveAsTable(\"pathirippilly_db.orders_sum_to_customer_product\")\n",
    "orders_sum_to_customer_status.write.mode(\"overwrite\").saveAsTable(\"pathirippilly_db.orders_sum_to_customer_status\")\n",
    "orders_exception.write.mode(\"overwrite\").saveAsTable(\"pathirippilly_db.orders_exception\")\n",
    "orders_ex_sum_to_prod.write.mode(\"overwrite\").saveAsTable(\"pathirippilly_db.orders_ex_sum_to_prod\")\n",
    "\n",
    "print(\"processing completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('/user/pathirippilly/test_data/testData.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|port|          timestamp|\n",
      "+----+-------------------+\n",
      "|9200|2020-06-19 02:12:41|\n",
      "|9200|2020-06-19 03:54:23|\n",
      "|  51|2020-06-19 05:32:11|\n",
      "|  22|2020-06-20 06:07:43|\n",
      "|  22|2020-06-20 01:11:12|\n",
      "|  51|2020-06-20 07:38:49|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|      date|port|count|\n",
      "+----------+----+-----+\n",
      "|2020-06-19|  51|    1|\n",
      "|2020-06-19|9200|    2|\n",
      "|2020-06-20|  22|    2|\n",
      "|2020-06-20|  51|    1|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(F.to_date('timestamp').alias('date'),'port').count().orderBy('date','port').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_sum_to_product.write.format('avro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
